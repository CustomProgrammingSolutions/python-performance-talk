<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Optimizing Python code

Janis Lesinskis

Slides: https://github.com/customprogrammingsolutions/python-performance-talk


---

# An example

We have a bunch of repositories and we want to calculate some stats from them.

For example, say we want to know what the average number of commits people make on repositories with which they are
regular contributors?

Meaning that we need to filter out the repositories where they have not made many commits.

---

# An example of correct but SLOW code
```python
import re; import json; import xml; import numpy; import tensorflow;
from .models import Repository, CommitCounts

def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)

authors_plus_commit_info = []; authors = {}
def averages_with_threshold(threshold: int) -> dict:
    authors_plus_commit_info.clear(); authors.clear()
    for commit_info in CommitCounts.objects.all():
        if commit_info.commit_count > threshold:
            authors_plus_commit_info.append((commit_info.author_name, commit_info.commit_count))
    for item in authors_plus_commit_info:
        try:
            authors[item[0]]
        except KeyError:
            authors[item[0]] = [item[1]]
        else:
            authors[item[0]].append(item[1])
    return {author: recursive_average(items) for author, items in authors.items()}
```
(Have a think about this, in the interim I'll talk about a few things.)

---

# First some nostalgia

The first computer I used I remember getting an error when trying to run a program "Floating point co processor not installed", this was macOS 7 in the 1990's. I didn't know what a float was back then! But I do remember having to install this:

![SoftwareFPU](softwareFPU.jpeg "from https://www.macintoshrepository.org/2639-softwarefpu-3-0")

---


# Performance of computers

FLOPS*






* Not the best metric of performance, but does show the order of magnitude changes that have occured in computing power.

???

Comparison of various performance
http://www.roylongbottom.org.uk/mips.htm

---

# Why peak FLOPS isn't the best metric

To get the maximum processing power is actually quite tough because the system goes only as fast as the bottlenecks.

CPython traditionally doesn't do well on CPU bound tasks

* GIL
* Poor locality of reference


???

The peak performance vs sustained performance is a huge factor in why supercomputing designs beat out some commodity hardware:
https://cug.org/5-publications/proceedings_attendee_lists/2003CD/S03_Proceedings/Pages/Authors/Muzio_slides.pdf


Commodity hardware optimized for benchmark results?

---

# Computers are fast these days!

Python is still slow relative to other languages but is fast enough to be viable for many production settings. But Python is fast to develop in!

As time goes on the computing power we have available to us has increased substantially. According to Moores Law\* computing power has been increasing exponentially as time goes on. However according to Wirth's Law\* software is getting slower than hardware is getting faster.

Given the speed of modern computers there is a general trend to optimize first for developer time, not execution time.
(When computers were less powerful you needed to write fairly highly optimized code to have any chance of it running fast enough to satisfy user requirements.)

\*(These are just observations and possibly somewhat self-fulfilling)

---

# Why do we optimize code

In a business sense we do it to make money.

One approach I've seen was just writing slow code and letting computing power catch up.

An AWS anecdote...

???

Talk about the paradigm shift of microcontroller work at Kanes vs the various one off internal apps that had no performance requirements at all.

---

# Why bother writing faster code then?

Bloat and other issues have meant that despite computers being vastly more powerful some tasks are actually getting *SLOWER* than they used to be!
(When you consider just how much faster computer have got this is actually quite staggering.)

Wirth wrote about this in his 1995 "A plea for lean software"

More recently I remember seeing [The Website Obesity Crisis](http://idlewords.com/talks/website_obesity.htm) by Maciej CegÅ‚owski

---

# The interaction of speed and development cost

Writing faster code is not a simple tradeoff of development costs vs speed.

Writing faster code can be easier than slower code, the example code for this talk took me a LOT longer to write slowly than doing it properly.

For example creating a faster build system can reduce overall development time.

We have pareto optimal situations and ones that are not.


---

# Different implementations

There's some options that will let you get some more speed without needing to change your code*.

If you are using CPython you may be able to switch to a more performant implementation like PyPy.

CPython is slow!

*Provided that the libraries are supported by the other implementation.

---

# PyPy

![PyPy Benchmarks](pypyBenchmarks.png "from https://speed.pypy.org")

---

#Back to the example code
```python
import re; import json; import xml; import numpy; import tensorflow;
from .models import Repository, CommitCounts

def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)

authors_plus_commit_info = []; authors = {}
def averages_with_threshold(threshold: int) -> dict:
    authors_plus_commit_info.clear(); authors.clear()
    for commit_info in CommitCounts.objects.all():
        if commit_info.commit_count > threshold:
            authors_plus_commit_info.append((commit_info.author_name, commit_info.commit_count))
    for item in authors_plus_commit_info:
        try:
            authors[item[0]]
        except KeyError:
            authors[item[0]] = [item[1]]
        else:
            authors[item[0]].append(item[1])
    return {author: recursive_average(items) for author, items in authors.items()}
```
---

# Computing an average

```python
def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)
```

Issues:

* Will fail if `len(items) > sys.maxrecursionlimit`
* SLOW! Function call overhead is expensive in Python (see zombie frames https://www.lesinskis.com/TIL_python_imports.html)
* Python does not optimize tail calls.
  * Memory grows as items length grows

???

Tail call recursion and Python: https://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion

Preserving accurate stack traces was seen to be more important than enabling this optimization. Explanation from Guido: http://neopythonic.blogspot.com.au/2009/04/final-words-on-tail-calls.html

CPython actually allocates a zombie stack frame when functions are first defined to speed up their execution. This can only really be done for the first frame because if it was done to arbitrary depths it would severly pessimize memory usage for all the functions that are non-recursive in nature. And in Python most functions are not recursive, this is a bit of a self-fulfilling prophecy but it is what it is.

---

# A faster way of computing the average

```python
def faster_average(items):
    """Compute the mean (average) for an iterable"""
    return sum(items) / len(items)
```

* We already know the length so we don't have to do `len` number of additions
* Fewer function calls


---

# The default I'd use in production

```python
import statistics
statistics.mean(items)
```

* Don't write code if you don't have to!
* But what if we know something specific about our data?
* The standard library has to be generic but *we* can be specific
  * `statistics.mean` handles edge cases we won't encounter (`Decimal` handling)

???

CPython statistics source: https://github.com/python/cpython/blob/d6debb24e06152a827769b0cac24c47deccdeac1/Lib/statistics.py

---

# Know your data!

Because we have defined the models we know that we are getting integers and only integers for commit counts:

```python
class CommitCounts(models.Model):
    """Respresent commit counts by author for a repo"""
    author_name = models.CharField(max_length=30)
    commit_count = models.IntegerField()
```

How can we use this knowledge to our advantage?

---

# Arrays



---

# Second example

Percentiles, graphs


How do we improve this?

---

# When algorithm choice matters it tends to matter a lot

This is not really Python specific but you will start to run into situations where algorithm choice matters
sooner in Python due to the slower execution speed.

We only needed to sort once here then take slices.

But if we are adding in items we'd have to sort very often on a potentially large data set...

https://pypi.org/project/sortedcontainers/

(There's probably a database implementation that can deal with this)

---


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
