<!DOCTYPE html>
<html>
  <head>
    <title>Optimizing Python talk - Janis Lesinskis</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }

table {
    border-collapse: collapse;
}
th, td {
    padding: 5px;
    border: 1px solid red;
}

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Optimizing Python code

Janis Lesinskis

Slides: https://github.com/customprogrammingsolutions/python-performance-talk

![CPS](logo-CPS.png)

---

# An example

We have a bunch of repositories and we want to calculate some stats from them.

For example, say we want to know what the average number of commits people make on repositories with which they are
regular contributors?

Meaning that we need to filter out the repositories where they have not made many commits.

---

# An example of correct but SLOW code
```python
import re; import json; import xml; import numpy; import tensorflow;
from .models import Repository, CommitCounts

def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)

authors_plus_commit_info = []; authors = {}
def averages_with_threshold(threshold: int) -> dict:
    authors_plus_commit_info.clear(); authors.clear()
    for commit_info in CommitCounts.objects.all():
        if commit_info.commit_count > threshold:
            authors_plus_commit_info.append((commit_info.author_name, commit_info.commit_count))
    for item in authors_plus_commit_info:
        try:
            authors[item[0]]
        except KeyError:
            authors[item[0]] = [item[1]]
        else:
            authors[item[0]].append(item[1])
    return {author: recursive_average(items) for author, items in authors.items()}
```
(Have a think about this, in the interim I'll talk about a few things.)

???

It was funny simultaneously doing TDD and other good practices while writing the slowest possible code I could that didn't involve esoteria.

---

# First some nostalgia

The first computer I used I remember getting an error when trying to run a program "Floating point co processor not installed", this was macOS 7 in the 1990's. I didn't know what a float was back then! But I do remember having to install this:

![SoftwareFPU](softwareFPU.jpeg "from https://www.macintoshrepository.org/2639-softwarefpu-3-0")


???


https://hackaday.com/2016/08/22/ask-hackaday-calling-all-68k-experts/ the motorola 68040 was the first chip in the apple series that had an inbuilt FPU. It appears that if you sent a floating point instruction to the CPU in this architecture and it couldn't handle it an ISR was fired that the operating system had to handle the floating point operations.

---

# Performance of computers

|processor                | MIPS   | FLOPS    |  year      |
|-------------------------|--------|----------|------------|
|AP-101B (Space shuttle)  |  0.4   |  ??      | 1981-1991  |
|Motorola  68040          |  18    |  ??      | 1990       |
|Cray 1                   |  160   |  160M    | 1976       |
|ARM Cortex A9 (Galaxy 2) |  7500  |  614M    | 2011       |
|Snapdragon 835 (Galaxy 8)|  ??    | 11.0G    | 2016       |
|Intel Core i7-6700K      | 207230 | 113.53G  | 2015       |

Not the best metric of performance, but does show the order of magnitude changes that have occured in computing power.

Storage has also got massively cheaper.

(If you wanted more floating point operations a modern GPU might be a good choice but Python doesn't run natively on GPUs)

???

Comparison of performance of various processors
https://history.nasa.gov/computers/Ch4-3.html
http://www.roylongbottom.org.uk/mips.htm
http://www.alternatewars.com/BBOW/Computing/Computing_Power.htm
https://techgage.com/article/intels-skylake-core-i7-6700k-a-performance-look/
http://iopscience.iop.org/article/10.1088/1742-6596/681/1/012049/pdf

---

# Why peak instructions per second isn't the best metric

Processors spend most of their time not at peak load as they can only go as fast as data is supplied to them, sustained perfomance tends to matter far more.
To get the maximum processing power is actually quite tough because the system goes only as fast as the bottlenecks.

CPython traditionally doesn't do well on CPU bound tasks because of bottlenecks:

* Global Interpreter Lock
* Poor locality of reference
* Paying for flexibility even when you don't need it


*Optimizing Python code is often about removing bottlenecks*

???


The peak performance vs sustained performance is a huge factor in why supercomputing designs beat out otherwise similar looking commodity hardware:
https://cug.org/5-publications/proceedings_attendee_lists/2003CD/S03_Proceedings/Pages/Authors/Muzio_slides.pdf

Not to mention the difference in Mean Time Before Failure, that report has a fairly striking part about the differences in MTBF between the various systems.

Is commodity hardware optimized for benchmark results?

---

# Computers are fast these days!

Python is still slow relative to other languages but is fast enough to be viable for many production settings. But Python is fast to develop in!

As time goes on the computing power we have available to us has increased substantially. According to Moores Law\* computing power has been increasing exponentially as time goes on. However according to Wirth's Law\* software is getting slower than hardware is getting faster.

Given the speed of modern computers there is a general trend to optimize first for developer time, not execution time.

\*(These are just observations and possibly somewhat self-fulfilling)

???

When computers were less powerful you needed to write fairly highly optimized code to have any chance of it running fast enough to satisfy user requirements. Excess compute power capacity is a very recent situation.


---

# Why do we optimize code

We want to reduce entropy.

A business *approximation* to this is to make money.

One approach I've seen was just writing slow code and letting computing power catch up.

An AWS anecdote...

???

Talk about the paradigm shift of microcontroller work vs the various one off internal apps that had no performance requirements at all.

---

# Why bother writing faster code then?

Essentially human time is the same as always, making people wait around for results is still undesirable.

Bloat and other issues have meant that despite computers being vastly more powerful some tasks are actually getting *SLOWER* than they used to be!
(When you consider just how much faster computer have got this is actually quite staggering.)

* Wirth wrote about this in his 1995 "A plea for lean software"
* [The Website Obesity Crisis](http://idlewords.com/talks/website_obesity.htm) by Maciej CegÅ‚owski

---

# The interaction of speed and development cost

Writing faster code is not a simple tradeoff of development costs vs speed.

Writing faster code can be easier than slower code, the example code for this talk took me a LOT longer to write slowly than doing it properly.

For example creating a faster build system can reduce overall development time.

We have pareto optimal situations and ones that are not.


---

# Different implementations

There's some options that will let you get some more speed without needing to change your code*.

If you are using CPython you may be able to switch to a more performant implementation like PyPy.

CPython is slow!

*Provided that the libraries are supported by the other implementation.

---

# PyPy

![PyPy Benchmarks](pypyBenchmarks.png "from https://speed.pypy.org")

---

#Back to the example code
```python
import re; import json; import xml; import numpy; import tensorflow;
from .models import Repository, CommitCounts

def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)

authors_plus_commit_info = []; authors = {}
def averages_with_threshold(threshold: int) -> dict:
    authors_plus_commit_info.clear(); authors.clear()
    for commit_info in CommitCounts.objects.all():
        if commit_info.commit_count > threshold:
            authors_plus_commit_info.append((commit_info.author_name, commit_info.commit_count))
    for item in authors_plus_commit_info:
        try:
            authors[item[0]]
        except KeyError:
            authors[item[0]] = [item[1]]
        else:
            authors[item[0]].append(item[1])
    return {author: recursive_average(items) for author, items in authors.items()}
```
---

# Computing an average

```python
def recursive_average(items, cumulative_sum=0, count=0):
    if not items:
        return cumulative_sum / count
    return recursive_average(items[1:], cumulative_sum+items[0], count+1)
```

Issues:

* Will fail if `len(items) > sys.getrecursionlimit()` (This was 1000 by default for me)
* SLOW! Function call overhead is expensive in Python (see zombie frames https://www.lesinskis.com/TIL_python_imports.html)
* Python does not optimize tail calls.
  * Memory grows as items length grows

???

Tail call recursion and Python: https://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion

Preserving accurate stack traces was seen to be more important than enabling this optimization. Explanation from Guido: http://neopythonic.blogspot.com.au/2009/04/final-words-on-tail-calls.html

CPython actually allocates a zombie stack frame when functions are first defined to speed up their execution. This can only really be done for the first frame because if it was done to arbitrary depths it would severly pessimize memory usage for all the functions that are non-recursive in nature. And in Python most functions are not recursive, this is a bit of a self-fulfilling prophecy but it is what it is.

---

# A faster way of computing the average

```python
def faster_average(items):
    """Compute the mean (average) for an iterable"""
    return sum(items) / len(items)
```

* We already know the length so we don't have to do `len` number of additions
* Fewer function calls


---

# The default I'd use in production

```python
import statistics
statistics.mean(items)
```

* Don't write code if you don't have to!
* But what if we know something specific about our data?
* The standard library has to be generic but *we* can be specific
  * `statistics.mean` handles edge cases we won't encounter (`Decimal` handling)

???

CPython statistics source: https://github.com/python/cpython/blob/d6debb24e06152a827769b0cac24c47deccdeac1/Lib/statistics.py

---

# Know your data!

Because we have defined the models we know that we are getting integers and only integers for commit counts:

```python
class CommitCounts(models.Model):
    """Represent commit counts by author for a repo"""
    author_name = models.CharField(max_length=30)
    commit_count = models.IntegerField()
```

How can we use this knowledge to our advantage?

---

# Optimizing for space with Arrays

```python
>>> import array; import sys
>>> a = array.array('i', range(1000))
>>> l = list(range(1000))
>>> sys.getsizeof(a)
4184
>>> sys.getsizeof(l)
9112
```
As you can see there are memory savings for the arrays as compared with the lists.
In some cases this can be fairly substantial.

---

# Benchmarks

Profiling with: https://github.com/jazzband/django-silk#installation


|function                 |100 items| 900 items| 10000items |
|-------------------------|---------|----------|------------|
|Recursive average        |  <1ms   |    1ms   |    NA      |
|Faster average           |  <1ms   | <1ms     |   1ms      |
|Standard library         |  <1ms   | 1ms      |   5ms      |

???
Silk profiles and stores the profiling data in the database and gives you a nice UI to look at how fast things were:

Data is cleared with `python manage.py silk_clear_request_log`

---

# Second part of the example

```python
import re; import json; import xml; import numpy; import tensorflow;
from .models import Repository, CommitCounts

authors_plus_commit_info = []; authors = {}
def averages_with_threshold(threshold: int) -> dict:
    authors_plus_commit_info.clear(); authors.clear()
    for commit_info in CommitCounts.objects.all():
        if commit_info.commit_count > threshold:
            authors_plus_commit_info.append((commit_info.author_name, commit_info.commit_count))
    for item in authors_plus_commit_info:
        try:
            authors[item[0]]
        except KeyError:
            authors[item[0]] = [item[1]]
        else:
            authors[item[0]].append(item[1])
    return {author: recursive_average(items) for author, items in authors.items()}
```

???
There's a variety of issues with this code, some of which are covered in the following slides.
One thing not covered is the global lookups, Python looks up variables from inner-most to outermost scope.

Setting a global variable as a local can save a bit of performance in the case of a heavily used operation, for example:

```python
def compute_maximums(self):
    maximums = []
    for items in big_list_of_lists:
        maximums.append(max(items))
    self.maximums = max(maximums)
```
To perform this operation max has to be looked up in the most local scope first because we could have reassigned something else to it.
In this instance the check is local to `compute_maximums` followed by the class scope this is in followed by the global scope.

We could do something like:
```python
def compute_maximums(self):
    _max = max
    maximums = []
    for items in big_list_of_lists:
        maximums.append(_max(items))
    self.maximums = _max(maximums)
```
And we might see a bit of a performance speedup, but this is really in the realm of micro-optimizations.

What's NOT a micro-optimization is that the reference to the global scope will interfere with optimizations that implementations like PyPy can perform.
So the optimizing JIT compiler that PyPy runs cannot safely perform all optimiz the results involving a global variable because the global could change between runs. This is a really substantial pessimization of performance in some cases. Not to mention it's bad coding practice to have a broader scope for variables than is needed.

---

# Unused imports

Python caches imports so the first time you import things it will load and subsequently it will load from cache.

Main issues with unused imports
* Ugly namespace clutter that can cause bugs
* Unnecessary memory usage
* Makes the startup time worse
  * This matters if you have Django spin up on demand (higher time to first byte)

???

Unnecessary importing is a huge problem in many situations because it slows down startup time. This is particularly noticeable in CLI applications that need to start up quickly.

Good conversation here: https://news.ycombinator.com/item?id=16978932

---

# First attempt at cleaning up

```python
def averages_with_threshold_fast(threshold: int) -> dict:
    commit_counts = defaultdict(list)
    for commit_info in CommitCounts.objects.filter(commit_count__gt=threshold):
        commit_counts[commit_info.author_name].append(commit_info.commit_count)
    return {author: faster_average(counts) for author, counts in commit_counts.items()}
```

Cleaner yes, but faster?

???

Note that while this has fewer lines of code and is easier to read this doesn't always outperform the "slow" code because it's a poor database query.

With performance don't assume, always profile. See slide before about using Silk to profile the code.

One thing that's very obvious though is that this code is easier to modify to be more performant (if necessary) because it's much cleaner code.
It is in much part cleaner _because_ of how it easier to maintain.

---

# When algorithm choice matters it tends to matter a lot

This is not really Python specific but you will start to run into situations where algorithm choice matters
sooner in Python due to the slower execution speed.

We only needed to sort once here then take slices.

However if we were frequently adding and removing items we'd have to sort very often on a potentially large data set...

https://pypi.org/project/sortedcontainers/

(There's probably a database implementation that can deal with this)

---

# Questions?

---
    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
